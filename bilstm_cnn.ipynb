{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_origin_data():\n",
    "    black_url = []\n",
    "    with open('data/anomal_url.txt') as f:\n",
    "        black_url = [url.strip() for url in f.readlines()]\n",
    "    white_url = []\n",
    "    with open('data/normal_traing_url.txt') as f:\n",
    "        white_url = [url.strip() for url in f.readlines()]\n",
    "    return black_url, white_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_url, white_url = load_origin_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25065, 36000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(black_url), len(white_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8080/tienda1/publico/anadir.jsp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_url[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8080/tienda1/publico/anadir.jsp?id=3&nombre=Vino+Rioja&precio=100&cantidad=55&B1=A%F1adir+al+carrito'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_url[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(black_url, white_url):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    texts = black_url + white_url\n",
    "    tokenizer = Tokenizer(char_level = True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(white_url + black_url)\n",
    "    X = pad_sequences(X, maxlen = 200)\n",
    "    y = np.array([0] * len(white_url) + [1] * len(black_url))\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(X, y) + [word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, word_index = load_data(black_url, white_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45798, 200), (15267, 200), (45798,), (15267,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(word_num):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Activation, BatchNormalization\n",
    "    from keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers.wrappers import Bidirectional\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(word_num, 100, input_length = 200))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.3)))\n",
    "    model.add(Conv1D(filters = 128, kernel_size = 3, padding = 'valid', activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(4))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    from keras import backend as K\n",
    "    def precision(y_true, y_pred):\n",
    "        true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        pred_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision_score = true_pos / (pred_pos + K.epsilon())\n",
    "        return precision_score\n",
    "    \n",
    "    def recall(y_true, y_pred):\n",
    "        true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        all_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall_score = true_pos / (all_pos + K.epsilon())\n",
    "        return recall_score\n",
    "    \n",
    "    def f1(y_true, y_pred):\n",
    "        precision_score = precision(y_true, y_pred)\n",
    "        recall_score = recall(y_true, y_pred)\n",
    "        f1_score = 2 * ((precision_score * recall_score) / (precision_score + recall_score))\n",
    "        return f1_score\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', \n",
    "                  metrics = [precision, recall, f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(len(word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 100)          4900      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200, 128)          84480     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 198, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 198, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 198, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 49, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 540,969\n",
      "Trainable params: 540,583\n",
      "Non-trainable params: 386\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, to_file = 'blstm-cnn.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train):\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "    model_checkpoint = ModelCheckpoint('model-blstm-cnn.h5', save_best_only = True, save_weights_only = True)\n",
    "    tensor_board = TensorBoard('tflog-blstm-cnn', write_graph = True, write_images = True)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs = 3, batch_size = 64, validation_split = 0.2, shuffle = True,\n",
    "             callbacks = [early_stopping, model_checkpoint, tensor_board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36638 samples, validate on 9160 samples\n",
      "Epoch 1/3\n",
      "11072/36638 [========>.....................] - ETA: 10:27 - loss: 0.5129 - precision: 0.6516 - recall: 0.8012 - f1: 0.7140"
     ]
    }
   ],
   "source": [
    "train(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
